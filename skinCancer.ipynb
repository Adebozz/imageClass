{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define some variables to store the dataset path, image size, and number of classes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"./skin-lesion-analysis-towards-melanoma-detection/\"\n",
    "IMG_SIZE = 100\n",
    "NUM_CLASSES = 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DATADIR variable should be replaced with the path to the dataset directory on your machine. The IMG_SIZE variable defines the size of the images after resizing. The NUM_CLASSES variable defines the number of classes in the dataset.\n",
    "\n",
    "Now, let's define a function to preprocess the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize the image to IMG_SIZE x IMG_SIZE\n",
    "    resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "    # Normalize the image to have pixel values between 0 and 1\n",
    "    normalized = resized / 255.0\n",
    "    # Reshape the image to have a single color channel\n",
    "    reshaped = np.reshape(normalized, (IMG_SIZE, IMG_SIZE, 1))\n",
    "    return reshaped\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes an image as input, converts it to grayscale, resizes it to IMG_SIZE x IMG_SIZE, normalizes it to have pixel values between 0 and 1, and reshapes it to have a single color channel.\n",
    "\n",
    "Now, let's load the images and labels from the dataset directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label in range(NUM_CLASSES):\n",
    "    path = os.path.join(DATADIR, str(label))\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        img_data = cv2.imread(img_path)\n",
    "        img_data = preprocess_image(img_data)\n",
    "        images.append(img_data)\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert the images and labels to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Shuffle the images and labels\n",
    "images, labels = shuffle(images, labels, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loops over each class in the dataset directory, loads each image, preprocesses it using the preprocess_image() function, and appends it to the images list along with its label. Finally, it converts the images and labels lists to NumPy arrays and shuffles them.\n",
    "\n",
    "Now, let's split the dataset into training and testing sets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the train_test_split() function from the sklearn library to split the images and labels arrays into training and testing sets, with 20% of the data used for testing.\n",
    "\n",
    "Next, let's define the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a sequential model with the following layers:\n",
    "\n",
    "1. `Conv2D` layer with 32 filters, kernel size of 3x3, ReLU activation function, and input shape of `IMG_SIZE x IMG_SIZE x 1`.\n",
    "2. `MaxPooling2D` layer with pool size of 2x2.\n",
    "3. `Conv2D` layer with 64 filters, kernel size of 3x3, ReLU activation function.\n",
    "4. `MaxPooling2D` layer with pool size of 2x2.\n",
    "5. `Conv2D` layer with 128 filters, kernel size of 3x3, ReLU activation function.\n",
    "6. `MaxPooling2D` layer with pool size of 2x2.\n",
    "7. `Flatten` layer to convert the output of the convolutional layers to a 1D array.\n",
    "8. `Dense` layer with 128 neurons and ReLU activation function.\n",
    "9. `Dense` layer with `NUM_CLASSES` neurons and softmax activation function.\n",
    "\n",
    "Finally, let's compile and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code compiles the model using the Adam optimizer, sparse categorical crossentropy loss function, and accuracy metric. It then trains the model for 10 epochs on the training data, with validation on the testing data.\n",
    "\n",
    "That's it! This program should be able to classify skin cancer images and non-cancer images using a CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
