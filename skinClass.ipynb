{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Importing necessary libraries\n",
    "First, we need to import the necessary libraries for our program. We'll be using TensorFlow and Keras for building and training our model, NumPy for data manipulation, and Matplotlib for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Loading the dataset\n",
    "Next, we need to load the dataset. We'll be using the \"Skin Cancer MNIST: HAM10000\" dataset, which contains 10,015 images of skin lesions, classified into 7 categories. You can download the dataset from Kaggle or from the following link: https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './skin-lesion-analysis-towards-melanoma-detection/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./skin-lesion-analysis-towards-melanoma-detection/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Load data from .npz file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(dataset_path)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Extract training and testing data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m x_train \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mx_train\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ADEBOSS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './skin-lesion-analysis-towards-melanoma-detection/'"
     ]
    }
   ],
   "source": [
    "# Change the path to the directory where the dataset is stored\n",
    "dataset_path = './skin-lesion-analysis-towards-melanoma-detection/'\n",
    "\n",
    "# Load the dataset\n",
    "# Load data from .npz file\n",
    "data = np.load(dataset_path)\n",
    "\n",
    "# Extract training and testing data\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Preprocessing the data\n",
    "Before training our model, we need to preprocess the data. In this case, we'll simply normalize the pixel values to be between 0 and 1, and reshape the images to be of size (28, 28, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape images\n",
    "x_train = x_train.reshape(-1, 28, 28, 3)\n",
    "x_test = x_test.reshape(-1, 28, 28, 3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Defining the model architecture\n",
    "Now we're ready to define our CNN architecture. We'll use the following layers:\n",
    "\n",
    "1. Conv2D layer with 32 filters, kernel size of (3, 3), ReLU activation, and padding='same'\n",
    "2. MaxPooling2D layer with pool size of (2, 2)\n",
    "3. Conv2D layer with 64 filters, kernel size of (3, 3), ReLU activation, and padding='same'\n",
    "4. MaxPooling2D layer with pool size of (2, 2)\n",
    "5. Conv2D layer with 128 filters, kernel size of (3, 3), ReLU activation, and padding='same'\n",
    "6. MaxPooling2D layer with pool size of (2, 2)\n",
    "7. Flatten layer\n",
    "8. Dense layer with 128 units and ReLU activation\n",
    "9. Dropout layer with rate of 0.5\n",
    "10. Dense layer with 7 units and softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(7)\n",
    "    (activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Compiling the model\n",
    "\n",
    "Next, we need to compile our model. We'll use categorical cross-entropy as the loss function, Adam as the optimizer, and accuracy as the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Training the model\n",
    "Now we're ready to train our model. We'll use a batch size of 32, and train for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Evaluating the model\n",
    "After training, we can evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Visualizing the results\n",
    "Finally, we can visualize the training and validation accuracy and loss over the epochs using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
